[
  {
    "objectID": "posts/making-an-avatar/index.html",
    "href": "posts/making-an-avatar/index.html",
    "title": "Creating a personal avatar",
    "section": "",
    "text": "A really quick post to shoutout the open-source libraries I used to create what I find to be gorgeous icons, e.g.¬†the favicon I use for my website, reproduced above in a larger size.\n\nboringavatars\ncoolors\n\nAnybody who worked with me previously will now know my secret to creating cute, company-themed user icons üòã\nimport Avatar from 'boring-avatars';\n\n&lt;Avatar \n        name='Julian do Nascimento Ricardo'\n        size='180'\n        variant='sunset'\n        colors={['#FB6107', '#F3DE2C', '#7CB518', '#5C8001', '#FBB02D']}\n/&gt;"
  },
  {
    "objectID": "posts/eaglei/intro/index.html#pilot-episode-updated-829",
    "href": "posts/eaglei/intro/index.html#pilot-episode-updated-829",
    "title": "Working with power outage data",
    "section": "Pilot episode (updated 8/29)",
    "text": "Pilot episode (updated 8/29)\nThis post is the first in a series where I will explore the DOE‚Äôs Eagle-I dataset on energy grid outages.\nIt‚Äôs a quick one, focusing on mise en place. I‚Äôll share some snippets of code for setting up a pipeline with the targets package to save compute time further downstream in the analysis, given the size of the data (available via figshare below). On my local machine, I will definitely save meaningful time by not loading the same csv multiple times. And especially during more exploratory phases of analysis, I enjoy fewer breaks for computation because I‚Äôll stay in a flow-ier state.\n\n\n\nFor setup preceding the steps I‚Äôll show, I recommend the targets walkthrough which runs through a minimally viable project.\nBefore showing snippets for tasks within the Eagle-I workflow, here‚Äôs the _targets.R script that connects the tasks together. On good days, and ideally always, I‚Äôll sketch out this script before coming up with code for the component tasks. In short, I want to load one year of Eagle-I data, then create monthly summaries of outage time & affected customers in every state and county. I‚Äôve chosen 2021 for reasons we‚Äôll get into on a future post.\n\nlist(\n  tar_target(\n    name = load_one_year,\n    command = load_eaglei_year(2021)\n  ),\n  tar_target(\n    name = state_month_hour,\n    command = summarise_mo_hr(load_one_year,\n                              c(\"state\", \"month\", \"hr\"))\n  ),\n  tar_target(\n    name = county_month_hour,\n    command = summarise_mo_hr(load_one_year,\n                              c(\"county\", \"state\", \"month\", \"hr\"))\n  )#,\n  ## IGNORE below for now\n  # tar_target(\n  #   name = ny_ecdf,\n  #   command = state_county_ecdf(county_month_hour,\n  #                               c(\"New York\"),\n  #                               c(\"Kings\", \"Erie\"))\n  # ),\n  # tar_target(\n  #   name = eia_ecdf,\n  #   command = state_ecdf(state_month_hour,\n  #                        c(\"Maine\",\n  #                          \"Texas\",\n  #                          \"West Virginia\",\n  #                          \"Mississippi\",\n  #                          \"Louisiana\",\n  #                          \"Michigan\",\n  #                          \"Kentucky\",\n  #                          \"Oregon\",\n  #                          # Least\n  #                          \"District of Columbia\",\n  #                          \"Delaware\",\n  #                          \"Florida\",\n  #                          \"North Dakota\",\n  #                          \"Nevada\"))\n  # )\n)\n\nFor the first few targets, I‚Äôll be using the here and tidytable packages to get my work done. The load_eaglei_year function will do little more than load the right subset of outage data, and the summarise_mo_hr function will do the heavy lifting for the two other targets we‚Äôll look at in the next installment.\n\n\nShoutout to Mark Fairbanks and the tidytable package!"
  },
  {
    "objectID": "posts/eaglei/intro/index.html#next-steps",
    "href": "posts/eaglei/intro/index.html#next-steps",
    "title": "Working with power outage data",
    "section": "Next steps",
    "text": "Next steps\nIn future posts, I‚Äôll cover individual targets in use here, then compare Eagle-I data with what the Energy Information Administration (EIA) visualized in this article. For now I‚Äôve copied the charts here for reference. The first shows how long the average customer deal with power interruptions from 2013 through 2021, split by whether a ‚Äúmajor event‚Äù triggered the interruption.\n\nThe second disaggregates the data further into points for each state, plotted by total interruption time (horizontal axis) and number of discrete interruptions (vertical)."
  },
  {
    "objectID": "posts/eaglei/intro/index.html#there-be-dragons",
    "href": "posts/eaglei/intro/index.html#there-be-dragons",
    "title": "Working with power outage data",
    "section": "There be dragons‚Ä¶",
    "text": "There be dragons‚Ä¶\nWith yet more installments, I‚Äôm debating whether to create a golem-based Shiny application or build a package for easier access to EAGLE-I data within the R universe.\n\nStick around to find out üò∏"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Who am I?",
    "section": "",
    "text": "I‚Äôm many things: data scientist, environmental engineer, son, brother, Brooklynite, runner, and trivia enthusiast. My specialties are understanding complex systems, especially involving energy & material flows, then developing reproducible models of them and turning results into actionable findings. I‚Äôm also motivated to mentor data scientists and grow organizations‚Äô analytic abilities.\nProfessionally, I‚Äôm a multi-lingual analyst with domain expertise in energy efficiency, data science, and program evaluation. I have consulted for energy & utilities clients, written peer-reviewed papers, and helped to grow analysis teams using R & Python.\nMy academic background is in physics, environmental engineering, and Spanish. Outside of coursework, I have also learned how to be a skilled educator, mostly by listening and being attentive to how others learn."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "These posts run through ETL of Eagle-ITM data from the Department of Energy and comparison with other data sources."
  },
  {
    "objectID": "blog.html#series",
    "href": "blog.html#series",
    "title": "Posts",
    "section": "",
    "text": "These posts run through ETL of Eagle-ITM data from the Department of Energy and comparison with other data sources."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Aligning with EIA‚Äôs metrics\n\n\n\n\n\n\n\n\nOct 11, 2024\n\n\n\n\n\n\n\nCreating a personal avatar\n\n\n\n\n\n\n\n\nOct 11, 2024\n\n\n\n\n\n\n\nWorking with power outage data\n\n\n\n\n\n\n\n\nAug 21, 2024\n\n\n\n\n\n\n\nHosting internal R packages on r-universe\n\n\n\n\n\n\n\n\nAug 15, 2024\n\n\n\n\n\n\n\nSo you‚Äôre probably wondering how I got to this post‚Ä¶\n\n\n\n\n\n\n\n\nAug 15, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "eaglei.html",
    "href": "eaglei.html",
    "title": "Series: Eagle-Itm",
    "section": "",
    "text": "Aligning with EIA‚Äôs metrics\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2024\n\n\n6 min\n\n\n\n\n\n\n\nWorking with power outage data\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2024\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/eaglei/eia_metrics/index.html",
    "href": "posts/eaglei/eia_metrics/index.html",
    "title": "Aligning with EIA‚Äôs metrics",
    "section": "",
    "text": "This post is the second in a series exploring the DOE‚Äôs Eagle-I dataset on energy grid outages. See previous posts here: first\nHowever, before going further, it‚Äôs worth digressing to pause and discuss differences in the data available from Eagle-I and that the Energy Information Administration (EIA) uses to visualize people‚Äôs experiences with power outages, in this article.\nThe main EIA metrics of interest include multiple ‚Äúduration‚Äù indices, and one ‚Äúfrequency‚Äù index:\n\nSystem Average Interruption Duration Index (SAIDI),\nSystem Average Interruption Frequency Index (SAIFI), and\nCustomer Average Interruption Duration Index (CAIDI).\n\nIn the article‚Äôs second figure (copied below), SAIDI and SAIFI show up on the horizontal and vertical axes, respectively.\n\n\nFor a quick-hit understanding, check the EIA‚Äôs video guide\n\nAll ths background to say that SAIDI divides the number of customers affected by an outage by the number of customers in the system. We do not have the latter available to us in Eagle-I, so for the sake of this exercise, I will pull in county-level population data from the tidycensus package, and see how closely we can replicate the EIA‚Äôs published values.\n\n\nAs always, there are important assumptions embedded in our assumptions. Here, we‚Äôre assuming everyone in every county in the US is connected to the grid, or ‚Äúin the system‚Äù, to use the EIA‚Äôs language. Not sure I like the sound of ‚Äúin the system‚Äù, but alas, we‚Äôre stuck with the term."
  },
  {
    "objectID": "posts/eaglei/eia_metrics/index.html#assembling-the-tools",
    "href": "posts/eaglei/eia_metrics/index.html#assembling-the-tools",
    "title": "Aligning with EIA‚Äôs metrics",
    "section": "",
    "text": "This post is the second in a series exploring the DOE‚Äôs Eagle-I dataset on energy grid outages. See previous posts here: first\nHowever, before going further, it‚Äôs worth digressing to pause and discuss differences in the data available from Eagle-I and that the Energy Information Administration (EIA) uses to visualize people‚Äôs experiences with power outages, in this article.\nThe main EIA metrics of interest include multiple ‚Äúduration‚Äù indices, and one ‚Äúfrequency‚Äù index:\n\nSystem Average Interruption Duration Index (SAIDI),\nSystem Average Interruption Frequency Index (SAIFI), and\nCustomer Average Interruption Duration Index (CAIDI).\n\nIn the article‚Äôs second figure (copied below), SAIDI and SAIFI show up on the horizontal and vertical axes, respectively.\n\n\nFor a quick-hit understanding, check the EIA‚Äôs video guide\n\nAll ths background to say that SAIDI divides the number of customers affected by an outage by the number of customers in the system. We do not have the latter available to us in Eagle-I, so for the sake of this exercise, I will pull in county-level population data from the tidycensus package, and see how closely we can replicate the EIA‚Äôs published values.\n\n\nAs always, there are important assumptions embedded in our assumptions. Here, we‚Äôre assuming everyone in every county in the US is connected to the grid, or ‚Äúin the system‚Äù, to use the EIA‚Äôs language. Not sure I like the sound of ‚Äúin the system‚Äù, but alas, we‚Äôre stuck with the term."
  },
  {
    "objectID": "posts/eaglei/eia_metrics/index.html#calculating-saidi",
    "href": "posts/eaglei/eia_metrics/index.html#calculating-saidi",
    "title": "Aligning with EIA‚Äôs metrics",
    "section": "Calculating SAIDI",
    "text": "Calculating SAIDI\nHere‚Äôs an example of a function that takes our 15min-interval Eagle-I data and calculates a SAIDI metric. Along the way, it creates a variable for the numerator in the SAIDI formula (total number of customer hours without power), joins the necessary census data for the denominator, then converts time intervals into the proper units for SAIDI in the concluding mutate call. Giving the function a meaningful name will make it easier to understand what the targets pipeline is doing.\n\ncalc_saidi &lt;- function(eaglei_df,\n                       census_df,\n                       # Time intervals are 15min per documentation, or 0.25h\n                       data_interval = as.numeric(0.25, units = \"hours\"),\n                       summ_by) {\n  \n  eaglei_df %&gt;%\n    summarise(\n      # Within each outage interval, how many customers were affected\n      tot_cust_hrs = sum(customers_out),\n      # whether grouped by state, state+county, etc.\n      .by = all_of(summ_by)\n    ) %&gt;% \n    # Join the relevant population data (may need to add year to possible vars)\n    join_eaglei_census(census_df,\n                       join_spec = intersect(summ_by,\n                                             c(\"state\", \"county\"))) %&gt;% \n    # Turn number of intervals into units of time (hours by default)\n    mutate(\n      across(c(tot_cust_hrs), \\(x) { x * data_interval}),\n      # Then calculate SAIDI\n      saidi = tot_cust_hrs / pop\n    )\n}\n\nSee the screengrab below for sample output from this function:\n\nIn _targets.R, we call the function in its own target, using the feature-enhanced Eagle-I dataset in the preceding target.\n\nlist(\n  # Targets before...,\n  tar_target(\n    name = add_features,\n    command = add_outage_id(states_eaglei)\n  ),\n  tar_target(\n    name = county_saidi,\n    command = calc_saidi(add_features,\n                         states_census,\n                         summ_by = c(\"state\", \"county\"))\n  ),\n  tar_target(\n    name = state_saidi,\n    command = calc_saidi(add_features,\n                         states_census,\n                         summ_by = c(\"state\"))\n  )\n  # Targets after...\n)\n\nThe state SAIDI target produces similar results, but fewer rows (one per state). They don‚Äôt exactly line up with the EIA chart, but states‚Äô relative performances are close enough to justify further investigation.\n."
  },
  {
    "objectID": "posts/eaglei/eia_metrics/index.html#saifi-caidi",
    "href": "posts/eaglei/eia_metrics/index.html#saifi-caidi",
    "title": "Aligning with EIA‚Äôs metrics",
    "section": "SAIFI & CAIDI",
    "text": "SAIFI & CAIDI\nI discussed metrics which I have not yet quantified. To recap, SAIFI is the number of interruptions the average customer experienced, and CAIDI (Customer Average Interruption Duration Index), the average length of each interruption.\nAt first I hoped to tackle multiple metrics with one function, but for now I‚Äôm opting to keep them separate. Calculating SAIFI (or CAIDI) may not end up looking too different from the SAIDI function, but I am leaving that (and potential consolidation of the functions) to future posts. Gotta stretch out the material ;)\nI will continue to document my work here, but superfans (lol) or future contributors can follow commits to the underlying repo as well!"
  },
  {
    "objectID": "posts/internal-pkg-r-universe/index.html",
    "href": "posts/internal-pkg-r-universe/index.html",
    "title": "Hosting internal R packages on r-universe",
    "section": "",
    "text": "This post attempts to replicate stock analysis with the coreStatsNMR package, available via the NMR Group r-universe. Specifically, using the statsTable function outlined in this post.\nFirst, downloading the library using the custom repos argument to point to the r-universe: install.packages(\"coreStatsNMR\", repos = c(\"https://nmrgroup.r-universe.dev\", \"https://cloud.r-project.org\"))\n\nlibrary(coreStatsNMR)\n\n\nAttaching package: 'coreStatsNMR'\n\n\nThe following object is masked from 'package:base':\n\n    mode\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nstatsTable(data = iris,\n           summVar = \"Sepal.Length\",\n           groupVar = \"Species\",\n           stats = c(\"n\", \"min\", \"max\", \"mean\", \"weighted.mean\", \"median\", \"sd\", \"iqr\", \"cv\"),\n           accuracy = 0.01,\n           drop0trailing = TRUE) %&gt;% \n  knitr::kable()\n\nWarning in statsTable.data.frame(data = iris, summVar = \"Sepal.Length\", : Using placeholder weights of 1 for all data\n\n\n\n\n\nstat\nsetosa\nversicolor\nvirginica\nTotal\n\n\n\n\nn\n50\n50\n50\n150\n\n\nmin\n4.30\n4.90\n4.90\n4.30\n\n\nmax\n5.80\n7.00\n7.90\n7.90\n\n\nmean\n5.01\n5.94\n6.59\n5.84\n\n\nweighted.mean\n5.01\n5.94\n6.59\n5.84\n\n\nmedian\n5.00\n5.90\n6.50\n5.80\n\n\nsd\n0.35\n0.52\n0.64\n0.83\n\n\niqr\n0.40\n0.70\n0.67\n1.30\n\n\ncv\n0.07\n0.09\n0.10\n0.14\n\n\n\n\n\n\nSo what?\nYay! We can run summary statistics on stock R data with our own package. Why do this? We already can write expressive pipelines with various packages: dplyr, data.table, collapse, or polars. The added value of a DIY function is not apparent, especially if it‚Äôs using those packages underneath.\nHowever, for a consulting firm, such as my previous employer, there is value in creating wrapped versions of the stock coreStats functions which incorporate project/client constraints and documentation. That way, the core functions‚Äô focus can be on being very good in a narrow scope (for each function), but they can be combined and/or extended via wrappers for projects and/or specific, repetitive applications. This does assume time is invested in designing them to play nicely with one another, and maintaining these conditions as the codebase evolves. Having shareable ‚Äúcore‚Äù functions separate from ‚Äúproject code‚Äù allows the firm to tap into additional marketing value as well, i.e.¬†more-visibly participating in open-source software (OSS) development.\nInternal and/or public packages are also ways to embed invaluable organizational knowledge, e.g.¬†in a package‚Äôs testing suite, warnings, errors, and documentation. Of course, embedding this knowledge requires caution so that only the sources/methods/etc appropriate for public use are exposed in public repos like the r-universe."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "So you‚Äôre probably wondering how I got to this post‚Ä¶",
    "section": "",
    "text": "First of all, welcome! This is my big break into the blogosphere, so you‚Äôre getting in on the ground floor. How exciting!\nThis is my place to get thoughts on the page, primarily in the realms of energy systems and data science. It‚Äôs also a place to showcase the skills I‚Äôve sharpened as a professional, and mess around with tools I‚Äôd like to add to my toolset.\nThis is also a place to start paying forward the knowledge sharing I benefited from at NMR Group, where I cut my teeth as an analyst, data scientist, and eventually as a mentor to folks starting their professional journeys in R and/or in data science."
  }
]